{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import standard word embeddings, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall\",\n",
    "\"he'll've\": \"he shall have\",\n",
    "\"he's\": \"he has\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall\",\n",
    "\"I'll've\": \"I shall have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.23764 ,  0.43119 , -0.72154 , -0.15513 ,  0.26631 , -0.4445  ,\n",
       "       -0.4452  , -0.66205 , -0.35055 ,  1.0197  , -0.91729 ,  0.20477 ,\n",
       "       -0.44747 ,  0.071965,  0.82335 , -0.023837,  0.76155 ,  0.9766  ,\n",
       "       -0.44837 , -0.7963  ,  0.027471,  1.0583  ,  0.41688 ,  0.73721 ,\n",
       "        1.1753  , -0.84602 , -1.1009  ,  0.68862 ,  1.2378  , -0.98452 ,\n",
       "        2.3631  ,  1.0793  , -0.15267 ,  0.13733 , -0.28105 ,  0.37881 ,\n",
       "       -0.15635 ,  0.47079 , -0.47013 , -0.53729 ,  0.20005 ,  0.16393 ,\n",
       "       -0.53282 ,  0.63922 ,  0.44527 , -0.10678 ,  0.44169 , -0.47774 ,\n",
       "        0.37379 ,  1.0782  ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['fun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_dict['fun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.zeros((len(embeddings_dict), len(embeddings_dict['fun'])), dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_map = {}\n",
    "for index,word in enumerate(embeddings_dict.keys()):\n",
    "    word_map[word] = index\n",
    "    W[index] = embeddings_dict[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly initialize M and calculate E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(W[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = len(W[0]) # Standard vector dimensions\n",
    "d = 25 # Required dimensions\n",
    "M = np.random.rand(k,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "vec_sigmoid = np.vectorize(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = vec_sigmoid(np.matmul(W,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 25)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Alignment loss in word space\n",
    "Selection of pivot and domain specific words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly,the candidate pool of pivot words is pinned to a set of common words that appear in more than theta_c documents in each domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the document\n",
    "# for each word in the document calculate its positive frequency and negative frequency.\n",
    "# first build a vocabulary of words\n",
    "\n",
    "# read the source document\n",
    "# make the vocabulary of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_df = pd.read_csv('books_labelled.txt', header = None, sep ='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = src_df.values[:,0]\n",
    "labels = src_df.values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "     def replace(match):\n",
    "         return contractions_dict[match.group(0)]\n",
    "     return contractions_re.sub(replace, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = expand_contractions(sentences[i]).lower().translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split(' '):\n",
    "        vocab.add(word)\n",
    "vocab.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_word_map = {}\n",
    "for index,word in enumerate(vocab):\n",
    "    src_word_map[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_word_freq = np.zeros((len(vocab),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence,label in zip(sentences,labels):\n",
    "    for word in sentence.split(' '):\n",
    "        if(word != ''):\n",
    "            src_word_freq[src_word_map[word],label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pool = set()\n",
    "theta_c = 5\n",
    "for word in vocab:\n",
    "    positive_instances = src_word_freq[src_word_map[word],1]\n",
    "    negative_instances = src_word_freq[src_word_map[word],0]\n",
    "    if(positive_instances >= theta_c and negative_instances >= theta_c):\n",
    "        candidate_pool.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1857"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidate_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(a,b):\n",
    "    total = a+b\n",
    "    sum1 = math.log(a/(a+b))\n",
    "    sum1 += math.log(b/(a+b))\n",
    "    return -sum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_entropy = {}\n",
    "for word in candidate_pool:\n",
    "    positive_instances = src_word_freq[src_word_map[word],1]\n",
    "    negative_instances = src_word_freq[src_word_map[word],0]\n",
    "    entropy = get_entropy(positive_instances, negative_instances)\n",
    "    cw_entropy[word] = entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_words = set()\n",
    "non_pivot = set()\n",
    "theta_b = 1.45\n",
    "for word in candidate_pool:\n",
    "    if(cw_entropy[word] > theta_b):\n",
    "        pivot_words.add(word)\n",
    "    else:\n",
    "        non_pivot.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pivot_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each pivot word select k non-pivot words that co-occur the most with it\n",
    "pivot_map = {}\n",
    "non_pivot_map = {}\n",
    "for index,word in enumerate(pivot_words):\n",
    "    pivot_map[word] = index\n",
    "    \n",
    "for index,word in enumerate(non_pivot):\n",
    "    non_pivot_map[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_co_freq = np.zeros((len(pivot_words), len(non_pivot)), dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    sent_pivot = set()\n",
    "    sent_non_pivot = set()\n",
    "    for word in sentence.split(' '):\n",
    "        if(word in pivot_words):\n",
    "            sent_pivot.add(word)\n",
    "        elif(word in non_pivot):\n",
    "            sent_non_pivot.add(word)\n",
    "    \n",
    "    for word in sent_pivot:\n",
    "        for word2 in sent_non_pivot:\n",
    "            pivot_co_freq[pivot_map[word], non_pivot_map[word2]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_pivot_rv_map = {}\n",
    "for word in non_pivot_map.keys():\n",
    "    non_pivot_rv_map[non_pivot_map[word]] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each pivot word get k non pivot words\n",
    "k = 5\n",
    "domain_specific_words = set()\n",
    "pivot_cor_k_words = {}\n",
    "for word in pivot_words:\n",
    "    indices = pivot_co_freq[pivot_map[word],:].ravel().argsort()[-k:]\n",
    "    pivot_cor_k_words[word] = [non_pivot_rv_map[index] for index in indices]\n",
    "    for word2 in [non_pivot_rv_map[index] for index in indices]:\n",
    "        domain_specific_words.add(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_specific_map = {}\n",
    "for index,word in enumerate(domain_specific_words):\n",
    "    domain_specific_map[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cell': ['and', 'my', 'the', 'phone', 'i'],\n",
       " 'do': ['and', 'it', 'to', 'the', 'i'],\n",
       " 'while': ['and', 'is', 'a', 'the', 'it'],\n",
       " 'these': ['of', 'to', 'the', 'and', 'i'],\n",
       " 'make': ['this', 'and', 'to', 'the', 'i'],\n",
       " 'there': ['it', 'i', 'is', 'the', 'a'],\n",
       " 'very': ['with', 'i', 'is', 'and', 'the'],\n",
       " 'good': ['i', 'it', 'a', 'and', 'the'],\n",
       " 'few': ['but', 'have', 'the', 'i', 'a'],\n",
       " 'not': ['this', 'and', 'it', 'the', 'i'],\n",
       " 'be': ['this', 'and', 'to', 'the', 'i'],\n",
       " 'enough': ['for', 'to', 'it', 'i', 'the'],\n",
       " 'did': ['this', 'that', 'it', 'i', 'the'],\n",
       " 'great': ['it', 'is', 'a', 'the', 'and'],\n",
       " 'quality': ['i', 'it', 'is', 'and', 'the'],\n",
       " 'camera': ['phone', 'and', 'is', 'the', 'a'],\n",
       " 'thing': ['was', 'that', 'it', 'i', 'the'],\n",
       " 'get': ['my', 'and', 'to', 'the', 'i'],\n",
       " 'only': ['but', 'a', 'i', 'it', 'the'],\n",
       " 'cheap': ['low', 'is', 'it', 'as', 'the'],\n",
       " 'comfortable': ['ear', 'and', 'is', 'the', 'it'],\n",
       " 'you': ['for', 'is', 'a', 'to', 'the'],\n",
       " 'problem': ['it', 'had', 'the', 'with', 'i'],\n",
       " 'at': ['and', 'it', 'i', 'a', 'the'],\n",
       " 'recommend': ['the', 'to', 'this', 'would', 'i'],\n",
       " 'charger': ['and', 'a', 'i', 'it', 'the'],\n",
       " 'small': ['as', 'a', 'is', 'it', 'and'],\n",
       " 'if': ['to', 'a', 'it', 'the', 'this'],\n",
       " 'better': ['a', 'the', 'and', 'it', 'i'],\n",
       " 'has': ['a', 'i', 'the', 'and', 'it'],\n",
       " 'still': ['it', 'this', 'the', 'and', 'i'],\n",
       " 'far': ['the', 'this', 'it', 'i', 'so'],\n",
       " 'reception': ['my', 'and', 'the', 'is', 'i'],\n",
       " 'volume': ['it', 'and', 'to', 'is', 'the'],\n",
       " 'when': ['is', 'i', 'phone', 'the', 'it'],\n",
       " 'design': ['a', 'to', 'i', 'is', 'the'],\n",
       " 'buy': ['product', 'and', 'it', 'phone', 'this'],\n",
       " 'device': ['in', 'and', 'i', 'the', 'this'],\n",
       " 'after': ['i', 'it', 'and', 'a', 'the'],\n",
       " 'little': ['but', 'is', 'my', 'this', 'a'],\n",
       " '1': ['it', 'is', 'of', '2', 'the'],\n",
       " 'days': ['it', 'a', 'i', 'the', 'and'],\n",
       " 'what': ['is', 'time', 'of', 'a', 'i'],\n",
       " 'over': ['that', 'this', 'phone', 'i', 'the'],\n",
       " 'does': ['i', 'phone', 'and', 'the', 'it'],\n",
       " 'sound': ['to', 'i', 'is', 'and', 'the'],\n",
       " 'headset': ['and', 'this', 'my', 'the', 'i'],\n",
       " 'easy': ['is', 'and', 'the', 'it', 'to'],\n",
       " 'been': ['with', 'the', 'phone', 'i', 'have'],\n",
       " 'out': ['a', 'it', 'of', 'and', 'the'],\n",
       " 'work': ['my', 'with', 'i', 'the', 'it'],\n",
       " 'charge': ['and', 'a', 'is', 'it', 'the'],\n",
       " 'got': ['the', 'it', 'i', 'a', 'and'],\n",
       " 'months': ['the', 'for', 'i', 'this', 'have'],\n",
       " 'voice': ['is', 'of', 'a', 'i', 'the'],\n",
       " 'bluetooth': ['it', 'the', 'i', 'a', 'my'],\n",
       " 'me': ['for', 'this', 'to', 'the', 'i'],\n",
       " 'than': ['more', 'it', 'and', 'the', 'i'],\n",
       " 'service': ['i', 'have', 'and', 'for', 'the'],\n",
       " 'or': ['phone', 'i', 'of', 'it', 'the'],\n",
       " 'ever': ['this', 'had', 'have', 'the', 'i'],\n",
       " 'tried': ['this', 'the', 'have', 'i', 'and'],\n",
       " 'could': ['a', 'and', 'my', 'the', 'i'],\n",
       " 'will': ['i', 'of', 'it', 'this', 'and'],\n",
       " 'any': ['have', 'phone', 'it', 'and', 'i'],\n",
       " 'year': ['this', 'my', 'and', 'the', 'a'],\n",
       " 'well': ['is', 'i', 'it', 'and', 'the']}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_cor_k_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = len(domain_specific_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.2 Pivot polarity graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_bag_of_words = np.zeros((len(sentences), len(vocab)), dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,sentence in enumerate(sentences):\n",
    "    for word in sentence.split(' '):\n",
    "        if(word != ''):\n",
    "            src_bag_of_words[index, src_word_map[word]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model.fit(src_bag_of_words, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.12187135e-01, -6.72938481e+10, -5.36313533e-02, ...,\n",
       "        9.85351037e+09,  2.26951558e+09,  4.86286584e+10])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity(a):\n",
    "    if(a>0):\n",
    "        return 1\n",
    "    elif(a<0):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_polarity = {}\n",
    "for word in pivot_words:\n",
    "    aI = lr_model.coef_[src_word_map[word]]\n",
    "    pivot_polarity[word] = polarity(aI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(pivot_words)+f\n",
    "R_matrix = np.zeros((size,size), dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in pivot_words:\n",
    "    for word2 in pivot_words:\n",
    "        R_matrix[pivot_map[word], pivot_map[word2]] = max(\n",
    "            [0,pivot_polarity[word]*pivot_polarity[word2]]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = len(pivot_words)\n",
    "for pivot in pivot_words:\n",
    "    pivot_index = pivot_map[pivot]\n",
    "    for word in pivot_cor_k_words[pivot]:\n",
    "        non_pivot_index = g+domain_specific_map[word]\n",
    "        R_matrix[pivot_index, non_pivot_index] = 1\n",
    "        R_matrix[non_pivot_index, pivot_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the p-matrix\n",
    "K_preplex = 50\n",
    "sigma_i = math.log(K_preplex)/math.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = pivot_words.union(domain_specific_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = np.zeros((len(total_words),len(total_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(word):\n",
    "    if word in pivot_words:\n",
    "        index = pivot_map[word]\n",
    "        return index\n",
    "    else:\n",
    "        index = g+domain_specific_map[word]\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_distance(word1, word2):\n",
    "    w1_vec = embeddings_dict[word1]\n",
    "    w2_vec = embeddings_dict[word2]\n",
    "    dist = np.linalg.norm((w1_vec-w2_vec),ord=2)**2\n",
    "    dist = math.exp(-dist/sigma_i)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word1 in total_words:\n",
    "    for word2 in total_words:\n",
    "        w1 = get_index(word1)\n",
    "        w2 = get_index(word2)\n",
    "        dist_matrix[w1,w2] = get_exp_distance(word1,word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_matrix = np.zeros((g+f,g+f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48770855812463654"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dist_matrix[0])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(g+f):\n",
    "    for j in range(g+f):\n",
    "        P_matrix[i,j] = dist_matrix[i,j]/(np.sum(dist_matrix[i]) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "R_mod = alpha*R_matrix+(1-alpha)*P_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.52520243e+00, 5.00791538e-01, 5.10646647e-01, ...,\n",
       "        2.23817105e-03, 4.00792677e-03, 4.24550749e-03],\n",
       "       [5.00056291e-01, 5.72907724e-01, 5.02048249e-01, ...,\n",
       "        2.39489434e-04, 2.36890737e-04, 4.85421816e-03],\n",
       "       [5.00363180e-01, 5.00982488e-01, 5.34971815e-01, ...,\n",
       "        6.69210744e-03, 2.89163679e-03, 7.30898069e-03],\n",
       "       ...,\n",
       "       [1.84688345e-04, 2.77887273e-04, 1.61882752e-02, ...,\n",
       "        8.45971714e-02, 1.09727475e-03, 2.64497997e-03],\n",
       "       [8.08138498e-04, 6.71661080e-04, 1.70923281e-02, ...,\n",
       "        2.68123692e-03, 2.06716739e-01, 3.46136952e-03],\n",
       "       [1.92324094e-04, 3.09214221e-03, 9.70626975e-03, ...,\n",
       "        1.45204560e-03, 7.77653319e-04, 4.64422990e-02]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.25961138,  1.48567906, -1.32466438, -0.87788263,  0.41942744,\n",
       "        1.86352817,  0.22411983, -0.13237484,  2.73876428,  1.07093671,\n",
       "        0.15576551,  0.60441445,  1.84034467,  1.68411009, -0.34509863,\n",
       "       -1.6438546 ,  1.79851692,  3.57073491, -0.21463936, -0.46626625,\n",
       "        0.40386841, -0.235728  , -0.64457899, -0.77657504, -0.94604055])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(embeddings_dict['word'],M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_E_exp_distance(word1, word2, M):\n",
    "    w1_vec = np.matmul(embeddings_dict[word1],M)\n",
    "    w2_vec = np.matmul(embeddings_dict[word2],M)\n",
    "    dist = np.linalg.norm((w1_vec-w2_vec),ord=2)**2\n",
    "    dist = math.exp(-dist/sigma_i)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_E_matrix = np.zeros((len(total_words),len(total_words)))\n",
    "for word1 in total_words:\n",
    "    for word2 in total_words:\n",
    "        w1 = get_index(word1)\n",
    "        w2 = get_index(word2)\n",
    "        dist_E_matrix[w1,w2] = get_E_exp_distance(word1,word2,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.336159359816297e-143"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(dist_E_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_matrix = np.zeros((g+f,g+f))\n",
    "for i in range(g+f):\n",
    "    for j in range(g+f):\n",
    "        Q_matrix[i,j] = dist_E_matrix[i,j]/(np.sum(dist_E_matrix[i]) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.77636851e+01, 4.75136977e-17, 1.15897291e-33, ...,\n",
       "        1.02925889e-89, 2.57043821e-03, 1.14105493e-40],\n",
       "       [3.35798650e-18, 1.96217268e+00, 1.61506097e-06, ...,\n",
       "        8.23822227e-36, 9.28586643e-14, 2.98740395e-08],\n",
       "       [2.56319769e-35, 5.05402774e-07, 6.14024819e-01, ...,\n",
       "        3.35538006e-15, 3.23110183e-25, 1.07679647e-02],\n",
       "       ...,\n",
       "       [1.39194662e-89, 1.57641785e-34, 2.05178031e-13, ...,\n",
       "        3.75469845e+01, 5.61867838e-72, 6.18145025e-11],\n",
       "       [3.99314682e-03, 2.04112887e-12, 2.26960322e-23, ...,\n",
       "        6.45422839e-72, 4.31305722e+01, 9.68554515e-31],\n",
       "       [7.67390513e-42, 2.84278301e-08, 3.27442227e-02, ...,\n",
       "        3.07398895e-12, 4.19300940e-32, 1.86718344e+00]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Neighbor loss in document space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
